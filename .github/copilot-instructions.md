# Cloudflare RAG Portfolio – AI coding guide

- Big picture: single Cloudflare Worker serves both API (Hono) and built React assets; start with [src/index.ts](src/index.ts) for routes and [src/patterns/basic-rag.ts](src/patterns/basic-rag.ts) for the RAG pipeline.
- Runtime/bindings: AI (Workers AI), D1 (`DATABASE`), Vectorize (`VECTOR_INDEX`), R2 (`ARTICLES_BUCKET`), KV caches, and Workflows (`INGESTION_WORKFLOW`) are configured in [wrangler.jsonc](wrangler.jsonc); Vectorize/D1/R2 are remote-only, `nodejs_compat` is enabled, and cron runs nightly at 02:00.
- Env knobs (wrangler vars): `LOG_LEVEL`, `ENABLE_TEXT_SPLITTING`, `DEFAULT_CHUNK_SIZE`/`DEFAULT_CHUNK_OVERLAP`, `DEFAULT_TOP_K`, `MAX_QUERY_LENGTH`, `CHAT_LOGGING_ENABLED`, and `CHAT_LOG_IP_SALT`.
- Dev loop: run `npm run dev` (Worker on 8787) and `npm run ui:dev` (Vite on 3000 with `/api` proxy per [ui/vite.config.ts](ui/vite.config.ts)); build UI with `npm run build` (outputs to `public/`).
- Deploy: `npm run deploy` builds UI, applies remote D1 migrations, then `wrangler deploy --env production`; migrations live in [migrations/](migrations/) and should be applied before code relying on them.
- Testing/tooling: `npm test` runs Vitest with Workers pool; `npm run cf-typegen` updates Cloudflare types.
- RAG flow: [basicRAG](src/patterns/basic-rag.ts) embeds the question with `@cf/baai/bge-base-en-v1.5`, queries Vectorize, fetches chunk text from D1 via the document store, builds a numbered context, and answers with `@cf/meta/llama-3.1-8b-instruct` using a strict citation-required system prompt (temperature 0, max_tokens 1024).
- Retrieval details: `topK` defaults to `env.DEFAULT_TOP_K`; optional `minSimilarity` filters matches; question length is capped by `env.MAX_QUERY_LENGTH`; empty retrieval returns a stock "not enough information" answer with no sources.
- Storage abstraction: [src/utils/document-store.ts](src/utils/document-store.ts) wraps R2 article storage (`articles/{id}.json`), D1 documents/chunks joins, and Vectorize upsert/query; batch IDs capped at 1000; vector queries request metadata.
- Ingestion workflow: [src/ingestion-workflow.ts](src/ingestion-workflow.ts) runs via Cloudflare Workflows – store article to R2, create document in D1, chunk text (langchain splitter), persist chunks, batch embeddings (size 10) with `@cf/baai/bge-base-en-v1.5`, and upsert vectors; respects `ENABLE_TEXT_SPLITTING`, `DEFAULT_CHUNK_SIZE`, and `DEFAULT_CHUNK_OVERLAP`.
- Chunking: [src/utils/chunking.ts](src/utils/chunking.ts) uses `RecursiveCharacterTextSplitter` with defaults 500/100 overlap and wiki-aware separators; metadata flags tables/lists; `estimateChunkCount` available for sizing.
- Chat logging: [src/utils/chat-logger.ts](src/utils/chat-logger.ts) writes sessions/messages/chunks to D1 if enabled; hashes IP with `CHAT_LOG_IP_SALT`; logging failures are non-fatal but expect tables from migrations (message/session/chunk tables).
- API surface: `/api/v1/query` GET/POST for questions and `/api/v1/ingest` for articles; `/api/v1/ingest/:workflowId` checks workflow status; `/health` and `/api/v1/docs` provide diagnostics, all defined in [src/index.ts](src/index.ts).
- Frontend: [ui/src/config.ts](ui/src/config.ts) uses relative API paths (same-origin in prod, proxied in dev); [ui/src/pages/BasicChatPage.tsx](ui/src/pages/BasicChatPage.tsx) issues GET `/api/v1/query?q=...` and renders citations.
- Data prep: fetch Wikipedia JSON with `python scripts/fetch-wikipedia.py --size-mb 10` into `data/wikipedia/` (format documented in [data/wikipedia/README.md](data/wikipedia/README.md)); ingest via `npm run ingest ./data/wikipedia [worker-url]` which batches POSTs to `/api/v1/ingest` per [scripts/ingest-wikipedia.ts](scripts/ingest-wikipedia.ts).
- Storage schema: D1 tables and FTS are defined in [migrations/](migrations/); vector metadata includes `documentId/chunkId/chunkIndex/title` (see `VectorMetadata` in [src/types/index.ts](src/types/index.ts)).
- Observability: structured logging via [src/utils/logger.ts](src/utils/logger.ts) with timers; `wrangler tail --env production` for live logs; ingest steps and chat logging emit contextual fields.
- Gotchas: Vectorize has no local mode—dev relies on remote resources; keep `public/` untracked (generated by UI build); ingestion workflow requires Worker to be running to accept POSTs.
